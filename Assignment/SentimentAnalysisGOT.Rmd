---
title: "Are we more civilized today?"
author: "Johanne Lucia Astrup Rasmussen"
date: " 19 march"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false 
      smooth_scroll: false 

    
---
## Loading the packages 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

#loading tidyverse library

library(tidyverse)
library(here)

# loading diffrent libraries for text mining:

library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

```

**Note:** for more text analysis, you can fork & work through Casey O’Hara and Jessica Couture’s eco-data-sci workshop (available here https://github.com/oharac/text_workshop)

### Getting the Game of Thrones book:
```{r get-document}

got_path <- here("data","got.pdf")
#creating a path and load the path into a new name 
got_text <- pdf_text(got_path)

#When I run "got_text" I can see whats inside the object. Running it will print the entire text, so beware. 

```

## Selecting a page 
```{r single-page}
#Each row is a page in my PDF-file. This code will select page 9 and only show that. 

#showing a single page of GoT
got_p9 <- got_text[9]
got_p9
```

## Splitting text into a line for each word:

```{r split-lines}
#I want to split the text, so every word has it's own line. Therefor I convert the vector to a dataframe. I am creating a new column using the mutate(). 
#stringr::str_split() breaks the pages up into individual lines. Thereafter I unnest into regular columns using `tidyr::unnest()`. In the end I remove white space with `stringr::str_trim()`

got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  #fjerner whitespace 
  mutate(text_full = str_trim(text_full)) 

#This code will show that the text has 30233 columns and 2 rows.Now I need to split it into single words 
got_df

```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

## Get the individual words (tokens) in tidy format

Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r tokenize}

#I use tidytext::unnest_tokens() to split columns into individual words. 

got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens

# Now each word has its own row!
# I unnested the words from our line. Exploding it into a new line. creating 151551 rows og 2 columns: all words are listed in tehir own individual line row. 

#count words 

got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc


```


## Removing stop words:

```{r stopwords}

#Now I want to remove the stopwords, so I can actually see the forest for bare threes. I use  `tidyr::anti_join()`: stop words to remove stop words. 

?stop_words #explains what the reason behind the stop_words list is 

view(stop_words)

#I am doing an antijoin. I am discarding the word and discarting the full text column 

got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)

got_stop
```

Now check the counts again: 
```{r count-words2}

#counting the words without the stop words 

got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)

got_swc
```

What if we want to get rid of all the numbers (non-text) in `got_stop`?
```{r skip-numbers}

#This code filters out numbers, as they aren't really capable of having a sentiment (directly at least).
# filter(is.na(as.numeric(word))) makes sure that only words that aren't capable of being converted into numbers are kept. 


#filtering the numbers away by filtering the "numeric"-values. 

got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))

#checking that the numbers are filtered away 

got_no_numeric
```

## A word cloud of Game of Thrones words (non-numeric)

See more: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

```{r wordcloud-prep}

# There are almost 2000 unique words 
#The unique function shows unique words. 

length(unique(got_no_numeric$word))

# We probably don't want to include them all in a word cloud. Therefor my filter only includes the top 100 most frequent. The count() will sort them from the most to the least frequent. 

got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)


got_top100

```

```{r wordcloud}

#This code creates a word cloud that shows the most common words in a plot. 

got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

#show the most common words in a word cloud plot  
got_cloud

```

## cosutimzing the word cloud 

```{r wordcloud-pro}
#This cloud costumies the word cloud so some words will appear bigger and have a color (it is highlighted). It basically changes the layout of the word cloud and makes it eaiser to understand and easier to interpret 

ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

Cool! And you can facet wrap (for different reports, for example) and update other aesthetics. See more here: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

## Sentiment analysis

“The three general-purpose lexicons are

  -  AFINN from Finn Årup Nielsen,
  -  bing from Bing Liu and collaborators, and
  -  nrc from Saif Mohammad and Peter Turney
  
## Afinn lexicon 
```{r afinn}

# I use AFINN, a lexicon with common words and their meaning, to see if the words are classified positive or negative. The words have scores running from -5 (negative) to 5 (positive). 

##WARNING## These collections include very offensive words. I urge you to not look at them in class.

get_sentiments(lexicon = "afinn")

# Look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos
```

## Bing lexicon 

bing: binary, "positive" or "negative"
```{r bing}

get_sentiments(lexicon = "bing")

```

## nrc lexicon 

```{r nrc}
get_sentiments(lexicon = "nrc")
```

I will do the sentiment analysis on the GOT text using afinn, and nrc. 


## Sentiment analysis with afinn: 

First, bind words in `ipcc_stop` to `afinn` lexicon:
```{r bind-afinn}

#The ones that appear in both lexicons are now joined. 

got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  #counting the rating of words  
  count(value)

# Plot them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value)) +
  theme_bw()

#result: The plot shows us that there are more negative words than positive in the game of thrones books (twice as many negative than positive). The most extreme rated words are less common in the text, while there are a lot of words with the 0-score. This probably also indicates, that my stopword list wasn't entirely good enough to sort out ex. names. 

```
## checking words based on their rate 

Investigate some of the words in a bit more depth:
```{r afinn-2}

# I check what words with the rate "2" appears: 
got_afinn2 <- got_afinn %>% 
  filter(value == 2)

#tænk over hvilke dele af rapporten der er brugbar. Hvis den starter med at takke samarbejdspartnere, bonger det ud som positiv, slevom rapporten måske er negativ i sit egentlige indhold. 

got_afinn2 %>% 
  distinct(word)

```

```{r afinn-2-more}

# Check the unique 2-score words:
unique(got_afinn2$word)

# Count & plot
got_afinn2_n <- got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  top_n(20, n) %>% #selecting only the top 20 most frequent words in GOT. Used chatGPT to find this small piece of code, as the graph was otherwise unreadable. 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  theme_bw()

# Is "Honor" necessarily positive? 

#Can we be sure that "Honor" is used positive in the text? What are the context?
```

Look back at the IPCC report, and search for "confidence." Is it typically associated with emotion, or something else? 

We learn something important from this example: Just using a sentiment lexicon to match words will not differentiate between different uses of the word...(ML can start figuring it out with context, but we won't do that here).

Or we can summarize sentiment for the report: 
```{r summarize-afinn}

#calculatin the mean and median. 

got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  
  )

got_summary 

#Result: the result indicate negative sentiments based on Afinn 
```

The mean and median indicate *slightly* positive overall sentiments based on the AFINN lexicon. 

### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the GOT non-stopword text with the nrc lexicon: 

```{r bind-bing}
# dividing words into sentiments. NRC is checking the words from the GOT-text to see the sentiment tied to the words. 

got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

## Removing words in the GOT text 

```{r check-exclusions}

# as I mentioned earlier in the word cloud code chunk, we see a lot of neutral words (especially names). I want to remove those. 

got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)

# Count to find the most excluded:

got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

#give me a glimpse: 

head(got_exclude_n)

##For example "ser" and "hand" are not coded with a sentiment. They are neutral (0)
```

## Visualizations 

Now find some counts: 
```{r count-bing}

#counting how many times different sentiments occur in the text. Then it sorts them in the most frequent. 

got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# Plotting my counts to make a visualization

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()



```

Or count by sentiment *and* word, then facet:

```{r count-nrc}

# A plot that shows the 5 most common words in each sentiment. 

got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

#plotting it in a ggplot with columns

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("figures","got_nrc_sentiment.png"), 
       height = 10, 
       width = 5)

#shows the 5 most common words in each sentiment. 

```

## What sentiment is tied to what Word? 

```{r nrc-confidence}

#Here we can check what sentiment a certain words is tied to. 

word_sentiment <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "stone")

# Yep, check it out:
word_sentiment

#This shows that "Stone" is tied to anger. But is that really so? 


#The big takeaway of sentiment analysis: Text mining without reading the paper 
```

## Big picture takeaway

There are serious limitations of sentiment analysis using existing lexicons, and you should **think really hard** about your findings and if a lexicon makes sense for your study. Otherwise, word counts and exploration alone can be useful! 

## Your task

Taking this script as a point of departure, apply sentiment analysis on the Game of Thrones. You will find a pdf in the data folder. What are the most common meaningful words and what emotions do you expect will dominate this volume? Are there any terms that are similarly ambiguous to the 'confidence' above? 

### Credits: 
This tutorial is inspired by Allison Horst's Advanced Statistics and Data Analysis.

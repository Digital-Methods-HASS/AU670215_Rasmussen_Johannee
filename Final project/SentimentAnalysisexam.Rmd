---
title: "Sentiment Analysis of Treaties"
author: "Johanne Lucia Astrup Rasmussen"
date: " 20 maj 2025"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false 
      smooth_scroll: false 

    
---
### Loading the packages for my project

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

#loading tidyverse library

library(tidyverse)
library(here)

# loading different libraries for text mining:

library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud) 

```

### Getting the The Treaties:
```{r get-document}

versailles_path <- here("data","TheTreatyOfVersailles.pdf")

#creating a path and load the path into a new name 
versailles_text <- pdf_text(versailles_path)

Paris_path <- here("data","Paris.pdf")
Paris_text <- pdf_text(Paris_path)

Tordesillas_path <- here("data","Tordesillas.pdf")
Tordesillas_text <- pdf_text(Tordesillas_path)

Westphalia_path <- here("data","Westphalia.pdf")
Westphalia_text <- pdf_text(Westphalia_path)

```


### Splitting text into a line for each word:

```{r split-lines}

#I want to split the text, so every word has it's own line. Therefor I convert the vector to a dataframe. I am creating a new column using the mutate(). stringr::str_split() breaks the pages up into individual lines. Thereafter I unnest into regular columns using `tidyr::unnest()`. In the end I remove white space with `stringr::str_trim()`

versailles_df <- data.frame(versailles_text) %>% 
  mutate(text_full = str_split(versailles_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  #fjerner whitespace 
  mutate(text_full = str_trim(text_full)) 

Paris_df <- data.frame(Paris_text) %>% 
  mutate(text_full = str_split(Paris_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

Tordesillas_df <- data.frame(Tordesillas_text) %>% 
  mutate(text_full = str_split(Tordesillas_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

Westphalia_df <- data.frame(Westphalia_text) %>% 
  mutate(text_full = str_split(Westphalia_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

versailles_df
Paris_df
Tordesillas_df
Westphalia_df


```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Getting the individual words in tidy format

```{r tokenize}

#Creating a line for each row: I use tidytext::unnest_tokens() to split columns into individual words. I unnested the words from our line. Exploding it into a new line, creating 151551 rows og 2 columns: all words are listed on their own individual line row. 

versailles_tokens <- versailles_df %>% 
  unnest_tokens(word, text_full)
#versailles_tokens

#count words to check 

versailles_wc <- versailles_tokens %>% 
  count(word) %>% 
  arrange(-n)
versailles_wc

Paris_tokens <- Paris_df %>% 
  unnest_tokens(word, text_full)
Paris_tokens

Paris_wc <- Paris_tokens %>% 
  count(word) %>% 
  arrange(-n)
Paris_wc

Tordesillas_tokens <- Tordesillas_df %>% 
  unnest_tokens(word, text_full)
Tordesillas_tokens

Tordesillas_wc <- Tordesillas_tokens %>% 
  count(word) %>% 
  arrange(-n)
Tordesillas_wc

Westphalia_tokens <- Westphalia_df %>% 
  unnest_tokens(word, text_full)
Westphalia_tokens

Westphalia_wc <- Westphalia_tokens %>% 
  count(word) %>% 
  arrange(-n)
Westphalia_wc

```


### Removing stop words:

```{r stopwords}

#Now I want to remove the stopwords. I use `tidyr::anti_join()`: stop words to remove stop words. I am doing an antijoin: I am discarding the word and discarding the full text column.  

# Add manual stopwords
manual_stopwords <- tibble(word = c("king", "lord", "don"))

versailles_stop <- versailles_tokens %>% 
  anti_join(stop_words) %>% 
  anti_join(manual_stopwords) %>%
  select(-versailles_text)
versailles_stop

Paris_stop <- Paris_tokens %>% 
  anti_join(stop_words) %>% 
  anti_join(manual_stopwords) %>%
  select(-Paris_text)
Paris_stop

Tordesillas_stop <- Tordesillas_tokens %>% 
  anti_join(stop_words) %>% 
  anti_join(manual_stopwords) %>% 
  select(-Tordesillas_text)
Tordesillas_stop

Westphalia_stop <- Westphalia_tokens %>% 
  anti_join(stop_words) %>% 
  anti_join(manual_stopwords) %>%
  select(-Westphalia_text)
Westphalia_stop

#Manualy removing "lord", "king" and "don" as these are labeled with a sentiment, but that doesnt make sense in the comparison as other actors such as "germany" etc. are removed from the text. 









```

Now check the counts again: 
```{r count-words2}

#counting the words without the stop words to see if the code worked. 

versailles_swc <- versailles_stop %>% 
  count(word) %>% 
  arrange(-n)
versailles_swc

Paris_swc <- Paris_stop %>% 
  count(word) %>% 
  arrange(-n)
Paris_swc

Tordesillas_swc <- Tordesillas_stop %>% 
  count(word) %>% 
  arrange(-n)
Tordesillas_swc

Westphalia_swc <- Westphalia_stop %>% 
  count(word) %>% 
  arrange(-n)
Westphalia_swc


```


```{r removing-numbers}

#This code filters out numbers, as they aren't really capable of having a sentiment in this context. I use filter(is.na(as.numeric(word))) to make sure that only words that aren't capable of being converted into numbers are kept. 


#filtering the numbers away by filtering the "numeric"-values. 

versailles_no_numeric <- versailles_stop %>% 
  filter(is.na(as.numeric(word)))

Paris_no_numeric <- Paris_stop %>% 
  filter(is.na(as.numeric(word)))

Tordesillas_no_numeric <- Tordesillas_stop %>% 
  filter(is.na(as.numeric(word)))

Westphalia_no_numeric <- Westphalia_stop %>% 
  filter(is.na(as.numeric(word)))

#checking that the numbers are filtered away 

versailles_no_numeric

```


### nrc lexicon 

```{r nrc}

# The nrc lexicon can be used for categorizing basic words based on their emotions.

get_sentiments(lexicon = "nrc")
```

### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the GOT non-stopword text with the nrc lexicon: 

```{r bind-bing}

# I use the nrc lexicon to find out what feelings words are typically associated with and dividing words into sentiments. Here I combine the words with sentiments based on NRC. 

versailles_nrc <- versailles_stop %>% 
  inner_join(get_sentiments("nrc"))

Paris_nrc <- Paris_stop %>% 
  inner_join(get_sentiments("nrc"))

Tordesillas_nrc <- Tordesillas_stop %>% 
  inner_join(get_sentiments("nrc"))

Westphalia_nrc <- Westphalia_stop %>% 
  inner_join(get_sentiments("nrc"))


```


### Removing more words in the treaties  

```{r check-exclusions}

#Despite using a stopwordlist, we still see a lot of neutral words (especially names). I want to remove these. 

versailles_exclude <- versailles_stop %>% 
  anti_join(get_sentiments("nrc"))

Paris_exclude <- Paris_stop %>% 
  anti_join(get_sentiments("nrc"))

Tordesillas_exclude <- Tordesillas_stop %>% 
  anti_join(get_sentiments("nrc"))

Westphalia_exclude <- Westphalia_stop %>% 
  anti_join(get_sentiments("nrc"))

# Counting to find the most excluded:

versailles_exclude_n <- versailles_exclude %>% 
  count(word, sort = TRUE)

#give me a glimpse: 
head(versailles_exclude_n)


##For example "germany" are not coded with a sentiment. It is neutral (0)
```


### Visualization "raw"

```{r}
versailles_nrc_raw <- versailles_nrc %>% 
  count(sentiment, sort = TRUE)

# Plotting my counts to make a visualization

ggplot(data = versailles_nrc_raw, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()

###
Paris_nrc_raw <- Paris_nrc %>% 
  count(sentiment, sort = TRUE)

# Plotting my counts to make a visualization

ggplot(data = Paris_nrc_raw, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()

###
Tordesillas_nrc_raw <- Tordesillas_nrc %>% 
  count(sentiment, sort = TRUE)

# Plotting my counts to make a visualization

ggplot(data = Tordesillas_nrc_raw, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()

###
Westphalia_nrc_raw <- Westphalia_nrc %>% 
  count(sentiment, sort = TRUE)

# Plotting my counts to make a visualization

ggplot(data = Westphalia_nrc_raw, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()
```

```{r}

#ChatGPT suggested this code for me, when asking for help to making a facet

# Add a document label to each data frame
versailles_nrc_raw$document <- "Versailles"
Paris_nrc_raw$document <- "Paris"
Tordesillas_nrc_raw$document <- "Tordesillas"
Westphalia_nrc_raw$document <- "Westphalia"

# Combine into one data frame
all_sentiments_raw <- bind_rows(
  versailles_nrc_raw,
  Paris_nrc_raw,
  Tordesillas_nrc_raw,
  Westphalia_nrc_raw
)

# Create the plot
facet_sentiment_plot <- ggplot(all_sentiments_raw, aes(y = reorder(sentiment, n), x = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ document, scales = "free_y") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Sentiment Counts in raw Texts (NRC)",
       x = "Sentiment", y = "Count")

# Show plot
facet_sentiment_plot


ggsave(plot = facet_sentiment_plot, 
       here("figures","raw_sentiment.png"), 
       height = 5, 
       width = 10)

```



### Visualizations "sampled"


```{r}

#VERY IMPORTANT. The treaties have different lengths. Therefore, in order to compare the treaties, I use the sample() function to select 1000 random words from the texts, making the data more manageable for generalizations. I used Chat-gpt to help me with this sample-function: 



# 1. Versailles - Sample 1000 words and perform sentiment analysis
set.seed(123)  # for reproducibility
sampled_versailles <- versailles_no_numeric %>%
  sample_n(500, replace = TRUE)

# Join with NRC sentiment lexicon
versailles_nrc <- sampled_versailles %>%
  inner_join(get_sentiments("nrc"), by = "word")

# Count sentiment
versailles_nrc_n <- versailles_nrc %>% 
  count(sentiment, sort = TRUE)

# Plotting the counts for Versailles
ggplot(data = versailles_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment)) +
  theme_bw() +
  labs(title = "Sentiment in Sampled Versailles Text (NRC)",
       x = "Sentiment", y = "Count")

# Paris 
sampled_paris <- Paris_no_numeric %>%
  sample_n(500, replace = TRUE)

Paris_nrc <- sampled_paris %>%
  inner_join(get_sentiments("nrc"), by = "word")

Paris_nrc_n <- Paris_nrc %>% 
  count(sentiment, sort = TRUE)

ggplot(data = Paris_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment)) +
  theme_bw() +
  labs(title = "Sentiment in Sampled Paris Text (NRC)",
       x = "Sentiment", y = "Count")

# Tordesillas 
sampled_tordesillas <- Tordesillas_no_numeric %>%
  sample_n(500, replace = TRUE)

Tordesillas_nrc <- sampled_tordesillas %>%
  inner_join(get_sentiments("nrc"), by = "word")

Tordesillas_nrc_n <- Tordesillas_nrc %>% 
  count(sentiment, sort = TRUE)


ggplot(data = Tordesillas_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment)) +
  theme_bw() +
  labs(title = "Sentiment in Sampled Tordesillas Text (NRC)",
       x = "Sentiment", y = "Count")

# Westphalia 
sampled_westphalia <- Westphalia_no_numeric %>%
  sample_n(500, replace = TRUE)

Westphalia_nrc <- sampled_westphalia %>%
  inner_join(get_sentiments("nrc"), by = "word")

Westphalia_nrc_n <- Westphalia_nrc %>% 
  count(sentiment, sort = TRUE)

ggplot(data = Westphalia_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment)) +
  theme_bw() +
  labs(title = "Sentiment in Sampled Westphalia Text (NRC)",
       x = "Sentiment", y = "Count")



```

### A facet containing all 4 visulazations 
```{r}

#ChatGPT suggested this code for me, when asking for help to making a facet

# Add a document label to each sentiment data frame
versailles_nrc_n$document <- "Versailles"
Paris_nrc_n$document <- "Paris"
Tordesillas_nrc_n$document <- "Tordesillas"
Westphalia_nrc_n$document <- "Westphalia"

# Combine them
all_sentiments <- bind_rows(
  versailles_nrc_n,
  Paris_nrc_n,
  Tordesillas_nrc_n,
  Westphalia_nrc_n
)

# Create the plot
sample_sentiment_plot <- ggplot(all_sentiments, aes(y = reorder(sentiment, n), x = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ document, scales = "free_y") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Sentiment in Sampled Texts (NRC)",
       x = "Sentiment",
       y = "Count")

# Show the plot
sample_sentiment_plot

# Save the plot
ggsave(plot = sample_sentiment_plot, 
       here("figures","sample_sentiment.png"), 
       height = 5, 
       width = 10)

```



Or count by sentiment *and* word, then facet:

```{r count-nrc}

# A plot that shows the 5 most common words in each sentiment. 

versailles_nrc_n5 <- versailles_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

#plotting it in a ggplot with columns

versailles_nrc_gg <- ggplot(data = versailles_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
versailles_nrc_gg

# Save it
ggsave(plot = versailles_nrc_gg, 
       here("figures","versailles_nrc_sentiment.png"), 
       height = 10, 
       width = 5)

#shows the 5 most common words in each sentiment. 

Westphalia_nrc_n5 <- Westphalia_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

#plotting it in a ggplot with columns

Westphalia_nrc_gg <- ggplot(data = Westphalia_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
Westphalia_nrc_gg

# Save it
ggsave(plot = Westphalia_nrc_gg, 
       here("figures","Westphalia_nrc_sentiment.png"), 
       height = 10, 
       width = 5)





Paris_nrc_n5 <- Paris_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

#plotting it in a ggplot with columns

Paris_nrc_gg <- ggplot(data = Paris_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
Paris_nrc_gg

# Save it
ggsave(plot = Paris_nrc_gg, 
       here("figures","Paris_nrc_sentiment.png"), 
       height = 10, 
       width = 5)



Tordesillas_nrc_n5 <- Tordesillas_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

#plotting it in a ggplot with columns

Tordesillas_nrc_gg <- ggplot(data = Tordesillas_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
Tordesillas_nrc_gg

# Save it
ggsave(plot = Tordesillas_nrc_gg, 
       here("figures","Tordesillas_nrc_sentiment.png"), 
       height = 10, 
       width = 5)

```

```{r}
# Sample 1000 random words for each dataset before doing sentiment analysis

# Versailles - Sample 1000 random words and perform sentiment analysis
set.seed(123)  # for reproducibility
sampled_versailles <- versailles_nrc %>%
  sample_n(500, replace = TRUE)

# Count the top 5 most common words by sentiment
versailles_nrc_n5 <- sampled_versailles %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

# Plotting the results for Versailles
versailles_nrc_gg <- ggplot(data = versailles_nrc_n5, aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "Count", title = "Treaty of Versailles 1919")

# Show it
print(versailles_nrc_gg)

# Save it
ggsave(plot = versailles_nrc_gg, 
       here("figures", "versailles_nrc_sampled.png"), 
       height = 10, 
       width = 5)

# Westphalia - Sample 1000 random words and perform sentiment analysis
sampled_westphalia <- Westphalia_nrc %>%
  sample_n(500, replace = TRUE)

# Count the top 5 most common words by sentiment
Westphalia_nrc_n5 <- sampled_westphalia %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

# Plotting the results for Westphalia
Westphalia_nrc_gg <- ggplot(data = Westphalia_nrc_n5, aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "Count", title = "The Peace of Westphalia 1648")

# Show it
print(Westphalia_nrc_gg)

# Save it
ggsave(plot = Westphalia_nrc_gg, 
       here("figures", "Westphalia_nrc_sampled.png"), 
       height = 10, 
       width = 5)

# Paris - Sample 1000 random words and perform sentiment analysis
sampled_paris <- Paris_nrc %>%
  sample_n(500, replace = TRUE)

# Count the top 5 most common words by sentiment
Paris_nrc_n5 <- sampled_paris %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

# Plotting the results for Paris
Paris_nrc_gg <- ggplot(data = Paris_nrc_n5, aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "Count", title = "The Treaty of Paris 1783")

# Show it
print(Paris_nrc_gg)

# Save it
ggsave(plot = Paris_nrc_gg, 
       here("figures", "Paris_nrc_sampled.png"), 
       height = 10, 
       width = 5)

# Tordesillas - Sample 1000 random words and perform sentiment analysis
sampled_tordesillas <- Tordesillas_nrc %>%
  sample_n(500, replace = TRUE)

# Count the top 5 most common words by sentiment
Tordesillas_nrc_n5 <- sampled_tordesillas %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

# Plotting the results for Tordesillas
Tordesillas_nrc_gg <- ggplot(data = Tordesillas_nrc_n5, aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "Count", title = "Treaty of Tordesillas 1494")

# Show it
print(Tordesillas_nrc_gg)

# Save it
ggsave(plot = Tordesillas_nrc_gg, 
       here("figures", "Tordesillas_nrc_sampled.png"),
       height = 10, 
       width = 5)




```
